{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "aDC973Tu-vhc",
        "outputId": "3aae4b97-059f-4f5b-f89f-a4422fc4dfdb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1c0BXEuDy8Cmm2jfN0YYGkQxFZd2ZIoLg\n",
            "From (redirected): https://drive.google.com/uc?id=1c0BXEuDy8Cmm2jfN0YYGkQxFZd2ZIoLg&confirm=t&uuid=c1d2e4be-cbb5-4f20-bfe8-268c4cf51ef7\n",
            "To: /content/dataset.zip\n",
            "100%|██████████| 1.11G/1.11G [00:08<00:00, 133MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/dataset.zip'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Google Drive file ID and URL\n",
        "file_id = '1c0BXEuDy8Cmm2jfN0YYGkQxFZd2ZIoLg'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# Output file path\n",
        "output_file = '/content/dataset.zip'\n",
        "\n",
        "# Download the file\n",
        "gdown.download(url, output_file, quiet=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QQE9B2au-xQJ"
      },
      "outputs": [],
      "source": [
        "# Directory to extract the contents\n",
        "extract_dir = '/content/dataset/'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(output_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-Xvv8eZ_EKD",
        "outputId": "4e24a839-950f-4993-c87a-fdff82296792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Code Description\n",
        "\n",
        "This code implements a deep learning model for generating reports from medical images, specifically X-ray images. It leverages a combination of the T5 transformer model and a ResNet50 model for this task. Below is a high-level overview of the key components and functionalities of the code:\n",
        "\n",
        "## `R2GenModel` Class\n",
        "\n",
        "The `R2GenModel` class extends `torch.nn.Module` and integrates both a transformer model (T5) and a convolutional neural network (ResNet50) to generate textual reports from images.\n",
        "\n",
        "- **Initialization (`__init__` method)**:\n",
        "  - Loads the T5 tokenizer and model for text generation.\n",
        "  - Initializes a pre-trained ResNet50 model for visual feature extraction.\n",
        "  - Modifies the final fully connected layer of ResNet50 to match the T5 model's input size.\n",
        "  - Adds a dropout layer for regularization.\n",
        "\n",
        "- **Forward Pass (`forward` method)**:\n",
        "  - Extracts visual features from input images using the ResNet50 model.\n",
        "  - Applies dropout to the extracted features.\n",
        "  - Generates reports by passing both the text inputs and the visual features to the T5 model.\n",
        "  - Supports both training (with labels) and inference (without labels).\n",
        "\n",
        "- **Visual Feature Extraction (`extract_visual_features` method)**:\n",
        "  - Processes input images to extract visual features using the modified ResNet50 model.\n",
        "  - Adjusts the dimensions of the extracted features to match the expected input shape of the T5 model.\n",
        "\n",
        "- **Caption Generation (`generate_caption` method)**:\n",
        "  - Generates textual captions for given images and text inputs.\n",
        "  - Uses the T5 model to generate predictions based on the visual features and text inputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NzI1ss2gGkrH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Pooriya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, BertTokenizer\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "class R2GenModel(torch.nn.Module):\n",
        "    def __init__(self, model_name='t5-small', device='cuda', dropout_prob=0.1):\n",
        "        super(R2GenModel, self).__init__()\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "        self.device = device\n",
        "\n",
        "        # Add visual extractor (ResNet101)\n",
        "        self.visual_extractor = models.resnet101(pretrained=True).to(device)\n",
        "        self.visual_extractor.fc = torch.nn.Linear(self.visual_extractor.fc.in_features, self.model.config.d_model).to(device)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, images, labels=None):\n",
        "        visual_features = self.extract_visual_features(images)\n",
        "        visual_features = self.dropout(visual_features)  # Apply dropout\n",
        "        if labels is not None:\n",
        "            return self.model(input_ids=input_ids, attention_mask=attention_mask, encoder_outputs=(visual_features,), labels=labels)\n",
        "        else:\n",
        "            return self.model.generate(input_ids=input_ids, attention_mask=attention_mask, encoder_outputs=(visual_features,))\n",
        "\n",
        "    def extract_visual_features(self, images):\n",
        "        images = images.to(self.device)\n",
        "        # Use features from the second-to-last layer for pooling\n",
        "        visual_features = self.visual_extractor(images)\n",
        "        visual_features = visual_features.unsqueeze(1)  # Add sequence dimension\n",
        "        batch_size, dim = visual_features.size(0), visual_features.size(2)\n",
        "        visual_features = visual_features.expand(batch_size, 512, dim)  # Expand to the same sequence length\n",
        "        return visual_features\n",
        "\n",
        "    def generate_caption(self, input_ids, images, max_length=50):\n",
        "        visual_features = self.extract_visual_features(images)\n",
        "        generated_ids = self.model.generate(input_ids=input_ids, encoder_outputs=(visual_features,), max_length=max_length, num_beams=1)\n",
        "        generated_texts = [self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "        return generated_texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `ReportImageDataset` Class\n",
        "\n",
        "The `ReportImageDataset` class is a custom dataset class that prepares the data for training and evaluation.\n",
        "\n",
        "- **Initialization (`__init__` method)**:\n",
        "  - Takes lists of reports and corresponding image paths, along with the tokenizer and optional image transformations.\n",
        "  \n",
        "- **Length (`__len__` method)**:\n",
        "  - Returns the total number of samples in the dataset.\n",
        "  \n",
        "- **Item Retrieval (`__getitem__` method)**:\n",
        "  - Loads and preprocesses an image.\n",
        "  - Tokenizes the corresponding report text.\n",
        "  - Returns a dictionary containing the tokenized inputs, attention masks, and preprocessed images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nhT1hhmtGmhP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "class ReportImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, reports, image_paths, tokenizer, image_transform=None):\n",
        "        self.reports = reports\n",
        "        self.image_paths = image_paths\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_transform = image_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reports)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        report = self.reports[idx]\n",
        "        image_path = self.image_paths[idx]\n",
        "\n",
        "        # Load and preprocess image\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.image_transform:\n",
        "            image = self.image_transform(image)\n",
        "\n",
        "        # Tokenize report text\n",
        "        inputs = self.tokenizer.encode_plus(\"generate report: \" + report, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'image': image\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Function\n",
        "\n",
        "The `train` function trains the `R2GenModel` using the provided training and validation datasets.\n",
        "\n",
        "- **Parameters**:\n",
        "  - `model`: The `R2GenModel` instance to be trained.\n",
        "  - `train_dataset` and `val_dataset`: The datasets for training and validation.\n",
        "  - `batch_size`: Batch size for data loading.\n",
        "  - `num_epochs`: Number of epochs to train.\n",
        "  - `learning_rate`: Learning rate for the optimizer.\n",
        "\n",
        "- **Training Loop**:\n",
        "  - Loads the data in batches.\n",
        "  - Performs forward and backward passes.\n",
        "  - Optimizes the model parameters.\n",
        "  - Computes and prints average training and validation losses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QjTx1XxrGozX"
      },
      "outputs": [],
      "source": [
        "def train(model, train_dataset, val_dataset, batch_size=8, num_epochs=5, learning_rate=1e-4):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_train_loss = 0.0\n",
        "        model.train()\n",
        "        for batch in train_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            images = batch['image'].to(device)\n",
        "            labels = batch['input_ids'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask, images, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        print(f\"Epoch {epoch + 1}, Average Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Validation step\n",
        "        total_val_loss = 0.0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dataloader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                images = batch['image'].to(device)\n",
        "                labels = batch['input_ids'].to(device)\n",
        "\n",
        "                outputs = model(input_ids, attention_mask, images, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "        print(f\"Epoch {epoch + 1}, Average Validation Loss: {avg_val_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Function\n",
        "\n",
        "The `evaluate_model` function evaluates the trained model on a test dataset using various metrics.\n",
        "\n",
        "- **Process**:\n",
        "  - Generates reports for images in the test set.\n",
        "  - Computes evaluation metrics (BLEU, METEOR, ROUGE, precision, recall, and F1 score).\n",
        "\n",
        "## Metrics Calculation\n",
        "\n",
        "Various functions and libraries (e.g., `Rouge`, `sentence_bleu`, `meteor_score`) are used to calculate evaluation metrics for the generated reports against reference reports.\n",
        "\n",
        "- **calculate_scores**:\n",
        "  - Calculates BLEU-1, BLEU-4, METEOR, and ROUGE scores.\n",
        "  - Computes precision, recall, and F1 score for the generated reports.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmwePDl1Aql5",
        "outputId": "8d2b6a24-8556-4136-ac2b-ad2435a38d0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Pooriya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\Pooriya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from rouge import Rouge\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import torch\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "findings = ['Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
        "            'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
        "            'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
        "            'Support Devices', 'No Finding']\n",
        "\n",
        "def calculate_ce(reference, prediction):\n",
        "    reference_set = set(reference.split())\n",
        "    prediction_set = set(prediction.split())\n",
        "    true_positives = len(reference_set.intersection(prediction_set))\n",
        "    precision = true_positives / len(prediction_set) if prediction_set else 0\n",
        "    recall = true_positives / len(reference_set) if reference_set else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0\n",
        "    return precision, recall, f1_score\n",
        "\n",
        "def calculate_scores(references, predictions):\n",
        "    bleu_scores_1 = []\n",
        "    bleu_scores_4 = []\n",
        "    meteor_scores_list = []\n",
        "    rouge = Rouge()\n",
        "    rouge_scores = {'rouge-1': [], 'rouge-2': [], 'rouge-l': []}\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for ref, pred in zip(references, predictions):\n",
        "        if not pred:  # Skip empty predictions\n",
        "            continue\n",
        "\n",
        "        ref_tokens = ref.split()\n",
        "        pred_tokens = pred.split()\n",
        "\n",
        "        # BLEU-1\n",
        "        bleu_scores_1.append(sentence_bleu([ref_tokens], pred_tokens, weights=(1, 0, 0, 0), smoothing_function=SmoothingFunction().method1))\n",
        "\n",
        "        # BLEU-4\n",
        "        bleu_scores_4.append(sentence_bleu([ref_tokens], pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=SmoothingFunction().method1))\n",
        "\n",
        "        # METEOR\n",
        "        meteor_scores_list.append(meteor_score([ref_tokens], pred_tokens))  # Tokenize the inputs\n",
        "\n",
        "        # ROUGE\n",
        "        rouge_score = rouge.get_scores(pred, ref)\n",
        "        for key in rouge_scores:\n",
        "            rouge_scores[key].append(rouge_score[0][key]['f'])\n",
        "\n",
        "        # Precision, Recall, F1\n",
        "        precision, recall, f1_score = calculate_ce(ref, pred)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1_score)\n",
        "\n",
        "    avg_bleu_score_1 = np.mean(bleu_scores_1) if bleu_scores_1 else 0\n",
        "    avg_bleu_score_4 = np.mean(bleu_scores_4) if bleu_scores_4 else 0\n",
        "    avg_meteor_score = np.mean(meteor_scores_list) if meteor_scores_list else 0\n",
        "    avg_rouge_scores = {key: np.mean(value) if value else 0 for key, value in rouge_scores.items()}\n",
        "    avg_precision = np.mean(precisions) if precisions else 0\n",
        "    avg_recall = np.mean(recalls) if recalls else 0\n",
        "    avg_f1_score = np.mean(f1_scores) if f1_scores else 0\n",
        "\n",
        "    return avg_bleu_score_1, avg_bleu_score_4, avg_meteor_score, avg_rouge_scores, avg_precision, avg_recall, avg_f1_score\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    references = []\n",
        "    predictions = []\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            images = batch['image'].to(device)\n",
        "\n",
        "            generated_texts = model.generate_caption(input_ids, images, max_length=50)\n",
        "            for i, generated_text in enumerate(generated_texts):\n",
        "                reference_text = model.tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
        "                if generated_text:  # Check if generated text is not empty\n",
        "                    references.append(reference_text)\n",
        "                    predictions.append(generated_text)\n",
        "\n",
        "    bleu_scores_1, bleu_scores_4, meteor_scores, rouge_scores, avg_precision, avg_recall, avg_f1_score = calculate_scores(references, predictions)\n",
        "    return bleu_scores_1, bleu_scores_4, meteor_scores, rouge_scores, avg_precision, avg_recall, avg_f1_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Script\n",
        "\n",
        "The main script orchestrates the entire process:\n",
        "\n",
        "1. Loads the dataset annotations.\n",
        "2. Prepares image paths and corresponding reports for training, validation, and testing.\n",
        "3. Initializes and trains the `R2GenModel`.\n",
        "4. Evaluates the trained model on the test dataset and prints the evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KHtRli0-pI8",
        "outputId": "da21f77c-c453-4983-f448-7b5092a2658e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Pooriya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Pooriya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "from torchvision import transforms\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data_path = 'iu_xray/annotation.json'\n",
        "\n",
        "    with open(data_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    train_data = data['train']\n",
        "    test_data = data['test']\n",
        "    val_data = data['val']\n",
        "\n",
        "    train_image_paths = []\n",
        "    train_reports = []\n",
        "    test_image_paths = []\n",
        "    test_reports = []\n",
        "    val_image_paths = []\n",
        "    val_reports = []\n",
        "\n",
        "    for example in train_data:\n",
        "        for path in example['image_path']:\n",
        "            train_image_paths.append(os.path.join('iu_xray/images', path))\n",
        "            train_reports.append(example['report'])\n",
        "\n",
        "    for example in test_data:\n",
        "        for path in example['image_path']:\n",
        "            test_image_paths.append(os.path.join('iu_xray/images', path))\n",
        "            test_reports.append(example['report'])\n",
        "\n",
        "    for example in val_data:\n",
        "        for path in example['image_path']:\n",
        "            val_image_paths.append(os.path.join('iu_xray/images', path))\n",
        "            val_reports.append(example['report'])\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = R2GenModel(model_name='t5-small', device=device)\n",
        "    model.to(device)\n",
        "\n",
        "    image_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_dataset = ReportImageDataset(train_reports, train_image_paths, model.tokenizer, image_transform=image_transform)\n",
        "    test_dataset = ReportImageDataset(test_reports, test_image_paths, model.tokenizer, image_transform=image_transform)\n",
        "    val_dataset = ReportImageDataset(val_reports, val_image_paths, model.tokenizer, image_transform=image_transform)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Average Training Loss: 1.0764766708304063\n",
            "Epoch 1, Average Validation Loss: 0.7018391554420059\n",
            "Epoch 2, Average Training Loss: 0.5168435152145426\n",
            "Epoch 2, Average Validation Loss: 0.715324650141033\n",
            "Epoch 3, Average Training Loss: 0.4085166711201999\n",
            "Epoch 3, Average Validation Loss: 0.5013175357032467\n"
          ]
        }
      ],
      "source": [
        "train(model, train_dataset, val_dataset, batch_size=8, num_epochs=3, learning_rate=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'r2gen_model_Bert.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bdeFkbBE2or",
        "outputId": "bb31191e-60d3-4fe3-b3f5-13964c21a71a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU 1 Scores: 0.3663652680114728\n",
            "BLEU 4 Scores: 0.12613107410298469\n",
            "METEOR Scores: 0.36358966858002745\n",
            "ROUGE Scores: {'rouge-1': 0.5178948255791592, 'rouge-2': 0.24148664715275556, 'rouge-l': 0.4859611291511941}\n",
            "Precision: 0.525952098389345\n",
            "Recall: 0.4451575186537092\n",
            "F1 Score: 0.4683903115337337\n"
          ]
        }
      ],
      "source": [
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "bleu_scores_1, bleu_scores_4, meteor_scores, rouge_scores, avg_precision, avg_recall, avg_f1_score = evaluate_model(model, test_dataloader)\n",
        "\n",
        "print(\"BLEU 1 Scores:\", bleu_scores_1)\n",
        "print(\"BLEU 4 Scores:\", bleu_scores_4)\n",
        "print(\"METEOR Scores:\", meteor_scores)\n",
        "print(\"ROUGE Scores:\", rouge_scores)\n",
        "print(\"Precision:\", avg_precision)\n",
        "print(\"Recall:\", avg_recall)\n",
        "print(\"F1 Score:\", avg_f1_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Samples:\n",
            "\n",
            "Input Tensor Type: torch.int64, Shape: torch.Size([1, 512])\n",
            "Ground Truth: generate report : cardiac and mediastinal contours are within normal limits. the lungs are clear. bony structures are intact.\n",
            "Generated Caption: ['generate report : the heart is normal in size. the mediastinum is unremarkable. the lungs are clear. no focal consolidation, pneumothorax, or pleural effusion.']\n",
            "\n",
            "Input Tensor Type: torch.int64, Shape: torch.Size([1, 512])\n",
            "Ground Truth: generate report : pa and lateral views. the cardiomediastinal silhouette is normal. the lungs are clear. no effusions, consolidation or pneumothorax.\n",
            "Generated Caption: ['generate report : the heart is normal in size. the mediastinum is unremarkable. the lungs are clear. no focal consolidation, pneumothorax, or pleural effusion.']\n",
            "\n",
            "Input Tensor Type: torch.int64, Shape: torch.Size([1, 512])\n",
            "Ground Truth: generate report : low lung volumes are present. the heart size and pulmonary vascularity appear within normal limits. the lungs are free of focal airspace disease. no pleural effusion or pneumothorax is seen. mild degenerative changes are present in the spine.\n",
            "Generated Caption: ['']\n",
            "\n",
            "Input Tensor Type: torch.int64, Shape: torch.Size([1, 512])\n",
            "Ground Truth: generate report : the heart size and pulmonary vascularity appear within normal limits. the lungs are free of focal airspace disease. no pleural effusion or pneumothorax is seen. calcified granuloma is present in the right lung base. bibasilar bandlike opacities are present. the appearance xxxx scarring or atelectasis.\n",
            "Generated Caption: ['generate report : the heart is normal in size. the mediastinum is unremarkable. the lungs are clear. no focal consolidation, pneumothorax, or pleural effusion.']\n",
            "\n",
            "Input Tensor Type: torch.int64, Shape: torch.Size([1, 512])\n",
            "Ground Truth: generate report : the heart is normal in size. the mediastinum is unremarkable. the lungs are hyperinflated. there is biapical scarring. no acute infiltrate or pleural effusion seen.\n",
            "Generated Caption: ['generate report : the heart is normal in size. the mediastinum is unremarkable. the lungs are clear. no focal consolidation, pneumothorax, or pleural effusion.']\n",
            "\n",
            "Test Samples:\n",
            "\n",
            "Input Tensor Type: torch.int64, Shape: torch.Size([1, 512])\n",
            "Ground Truth: generate report : both lungs are clear and expanded. heart and mediastinum normal.\n",
            "Generated Caption: ['generate report : the heart is normal in size. the mediastinum is unremarkable. the lungs are clear. no focal consolidation, pneumothorax, or pleural effusion.']\n",
            "\n",
            "Input Tensor Type: torch.int64, Shape: torch.Size([1, 512])\n",
            "Ground Truth: generate report : no pneumothorax, pleural effusion, or focal airspace disease. heart size normal. stable cardiomediastinal silhouette. nodular opacities consistent with chronic granulomatous disease. bony structures intact.\n",
            "Generated Caption: ['generate report : the heart is normal in size. the mediastinum is unremarkable. the lungs are clear. no focal consolidation, pneumothorax, or pleural effusion.']\n",
            "\n",
            "Input Tensor Type: torch.int64, Shape: torch.Size([1, 512])\n",
            "Ground Truth: generate report : bilateral glenohumeral degenerative joint disease. scattered degenerative changes of the thoracic spine. stable mild heart enlargement. prominence of soft tissue density in the upper mediastinum. it is increased from most recent prior exam on xxxx. however, it appears similar compared to xxxx exams performed in xxxx. no focal area of consolidation, pleural effusion, or pneumothorax. focal opacity in the left upper lobe xxxx represents scarring or related to overlying rib opacity.\n",
            "Generated Caption: ['generate report : the heart is normal in size. the mediastinum is unremarkable. the lungs are clear. no focal consolidation, pneumothorax, or pleural effusion.']\n",
            "\n",
            "Input Tensor Type: torch.int64, Shape: torch.Size([1, 512])\n",
            "Ground Truth: generate report : pa and lateral views the chest were obtained. there are low lung volumes on the frontal view, which accentuates heart size and lung markings. the heart size is upper limits normal or mildly enlarged. mediastinum normal width. the pulmonary vasculature is within normal limits. there is left lung base atelectasis on frontal xxxx xxxx secondary to low volumes. no pneumothorax, pleural effusion, or focal air space consolidation.\n",
            "Generated Caption: ['generate report : the heart is normal in size. the mediastinum is unremarkable. the lungs are clear. no focal consolidation, pneumothorax, or pleural effusion.']\n",
            "\n",
            "Input Tensor Type: torch.int64, Shape: torch.Size([1, 512])\n",
            "Ground Truth: generate report : heart size within normal limits, stable mediastinal and hilar contours. no focal alveolar consolidation, no definite pleural effusion seen. no typical findings of pulmonary edema.\n",
            "Generated Caption: ['generate report : the heart size and mediastinal contours are within normal limits. the lungs are clear. no focal consolidation, pneumothorax, or pleural effusion.']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import torch\n",
        "\n",
        "def print_random_captions(model, train_dataset, test_dataset, num_samples=5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def get_random_samples(dataset, num_samples):\n",
        "        indices = torch.tensor(random.sample(range(len(dataset)), num_samples)).to(dtype=torch.long, device=device)\n",
        "        samples = [dataset[i] for i in indices]\n",
        "        return samples\n",
        "\n",
        "    train_samples = get_random_samples(train_dataset, num_samples)\n",
        "    test_samples = get_random_samples(test_dataset, num_samples)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        print(\"Train Samples:\\n\")\n",
        "        for sample in train_samples:\n",
        "            input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
        "            image = sample['image'].unsqueeze(0).to(device)\n",
        "            # Check input tensor type and shape\n",
        "            print(f\"Input Tensor Type: {input_ids.dtype}, Shape: {input_ids.shape}\")\n",
        "            ground_truth = model.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "            generated_caption = model.generate_caption(input_ids, image)  # Corrected here\n",
        "            print(f\"Ground Truth: {ground_truth}\")\n",
        "            print(f\"Generated Caption: {generated_caption}\\n\")\n",
        "\n",
        "        print(\"Test Samples:\\n\")\n",
        "        for sample in test_samples:\n",
        "            input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
        "            image = sample['image'].unsqueeze(0).to(device)\n",
        "            # Check input tensor type and shape\n",
        "            print(f\"Input Tensor Type: {input_ids.dtype}, Shape: {input_ids.shape}\")\n",
        "            ground_truth = model.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "            generated_caption = model.generate_caption(input_ids, image)  # Corrected here\n",
        "            print(f\"Ground Truth: {ground_truth}\")\n",
        "            print(f\"Generated Caption: {generated_caption}\\n\")\n",
        "print_random_captions(model, train_dataset, test_dataset)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
